{"run_name": "GraphReasoning_logiqa" ,
  "task_name": "GraphReasoning_logiqa" ,
  "model_name_or_path": "../huggingface/roberta-large" ,
  "data_dir":"data/logiqa",
  "do_train": false,
  "do_eval": true ,
  "seed": 123 ,
  "model_type": "Roberta" ,
  "max_seq_length": 384 ,
  "per_device_eval_batch_size": 16 ,
  "per_device_train_batch_size": 2 ,
  "gradient_accumulation_steps": 8 ,
  "num_train_epochs": 2 ,
  "output_dir": "checkpoints/logiqa" ,
  "logging_steps": 200 ,
  "learning_rate": 7e-6 ,
  "overwrite_output_dir": false ,
  "overwrite_cache": false,
  "resume_from_checkpoint": true,
  "evaluation_strategy": "steps"  ,
  "save_strategy": "steps" ,
  "eval_steps": 500,
  "save_steps": 500,
  "metric_for_best_model": "acc" ,
  "gnn_layers_num": 2 ,
  "save_total_limit": 2 ,
  "dropout": 0.1 ,
  "warmup_ratio": 0.1 ,
  "pooling_type": "attention_pooling_with_gru"
}